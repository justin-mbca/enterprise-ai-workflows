{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1bcd2d",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction - Complete MLOps Workflow\n",
    "\n",
    "**Simulates:** Azure ML experiment tracking and model management  \n",
    "**Tech Stack:** MLflow, Scikit-learn, Pandas\n",
    "\n",
    "This notebook demonstrates a complete ML lifecycle:\n",
    "1. Data preparation\n",
    "2. Experiment tracking\n",
    "3. Model training and evaluation\n",
    "4. Model registry\n",
    "5. Model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q mlflow scikit-learn pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df5bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e732ab19",
   "metadata": {},
   "source": [
    "## 1. Setup MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ddd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow\n",
    "MLFLOW_TRACKING_URI = \"http://mlflow:5000\"\n",
    "EXPERIMENT_NAME = \"customer-churn-prediction\"\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"ðŸ“Š MLflow Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "print(f\"ðŸ§ª Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"ðŸ”— MLflow UI: http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc587bfa",
   "metadata": {},
   "source": [
    "## 2. Generate Sample Dataset\n",
    "\n",
    "For this demo, we'll create a synthetic customer churn dataset.\n",
    "In production, you'd load real data from your data warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb84355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_churn_dataset(n_samples=5000, random_state=42):\n",
    "    \"\"\"Generate synthetic customer churn dataset\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Customer demographics\n",
    "    customer_ids = np.arange(1, n_samples + 1)\n",
    "    tenure = np.random.randint(1, 72, n_samples)  # Months\n",
    "    age = np.random.randint(18, 80, n_samples)\n",
    "    \n",
    "    # Contract details\n",
    "    contract_types = np.random.choice(\n",
    "        ['Month-to-month', 'One year', 'Two year'],\n",
    "        n_samples,\n",
    "        p=[0.5, 0.3, 0.2]\n",
    "    )\n",
    "    \n",
    "    # Services\n",
    "    internet_service = np.random.choice(\n",
    "        ['DSL', 'Fiber optic', 'No'],\n",
    "        n_samples,\n",
    "        p=[0.3, 0.5, 0.2]\n",
    "    )\n",
    "    \n",
    "    online_security = np.random.choice(['Yes', 'No'], n_samples, p=[0.3, 0.7])\n",
    "    tech_support = np.random.choice(['Yes', 'No'], n_samples, p=[0.3, 0.7])\n",
    "    streaming_tv = np.random.choice(['Yes', 'No'], n_samples, p=[0.4, 0.6])\n",
    "    \n",
    "    # Billing\n",
    "    monthly_charges = np.random.uniform(20, 120, n_samples)\n",
    "    total_charges = tenure * monthly_charges + np.random.normal(0, 50, n_samples)\n",
    "    total_charges = np.maximum(total_charges, 0)  # No negative charges\n",
    "    \n",
    "    payment_method = np.random.choice(\n",
    "        ['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'],\n",
    "        n_samples,\n",
    "        p=[0.4, 0.2, 0.2, 0.2]\n",
    "    )\n",
    "    \n",
    "    # Generate churn (target variable)\n",
    "    # Higher churn probability for:\n",
    "    # - Month-to-month contracts\n",
    "    # - High monthly charges\n",
    "    # - Low tenure\n",
    "    # - Electronic check payment\n",
    "    \n",
    "    churn_prob = np.zeros(n_samples)\n",
    "    \n",
    "    # Contract type influence\n",
    "    churn_prob += np.where(contract_types == 'Month-to-month', 0.3, 0)\n",
    "    churn_prob += np.where(contract_types == 'One year', 0.1, 0)\n",
    "    \n",
    "    # Tenure influence (inverse)\n",
    "    churn_prob += 0.3 * (1 - tenure / 72)\n",
    "    \n",
    "    # Monthly charges influence\n",
    "    churn_prob += 0.2 * (monthly_charges / 120)\n",
    "    \n",
    "    # Payment method influence\n",
    "    churn_prob += np.where(payment_method == 'Electronic check', 0.15, 0)\n",
    "    \n",
    "    # Service quality influence\n",
    "    churn_prob -= np.where(online_security == 'Yes', 0.1, 0)\n",
    "    churn_prob -= np.where(tech_support == 'Yes', 0.1, 0)\n",
    "    \n",
    "    # Add some randomness\n",
    "    churn_prob += np.random.normal(0, 0.1, n_samples)\n",
    "    churn_prob = np.clip(churn_prob, 0, 1)\n",
    "    \n",
    "    # Generate actual churn\n",
    "    churn = (np.random.random(n_samples) < churn_prob).astype(int)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'customer_id': customer_ids,\n",
    "        'tenure': tenure,\n",
    "        'age': age,\n",
    "        'contract_type': contract_types,\n",
    "        'internet_service': internet_service,\n",
    "        'online_security': online_security,\n",
    "        'tech_support': tech_support,\n",
    "        'streaming_tv': streaming_tv,\n",
    "        'monthly_charges': monthly_charges,\n",
    "        'total_charges': total_charges,\n",
    "        'payment_method': payment_method,\n",
    "        'churn': churn\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate dataset\n",
    "print(\"ðŸ”„ Generating dataset...\")\n",
    "df = generate_churn_dataset(n_samples=5000)\n",
    "print(f\"âœ… Generated {len(df)} customer records\")\n",
    "print(f\"ðŸ“Š Churn rate: {df['churn'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b3113c",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12827504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET INFO\")\n",
    "print(\"=\" * 60)\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ab715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize churn distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Churn distribution\n",
    "df['churn'].value_counts().plot(kind='bar', ax=axes[0, 0], color=['green', 'red'])\n",
    "axes[0, 0].set_title('Churn Distribution')\n",
    "axes[0, 0].set_xlabel('Churn (0=No, 1=Yes)')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# Churn by contract type\n",
    "pd.crosstab(df['contract_type'], df['churn']).plot(kind='bar', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Churn by Contract Type')\n",
    "axes[0, 1].set_xlabel('Contract Type')\n",
    "\n",
    "# Tenure distribution\n",
    "df[df['churn']==0]['tenure'].hist(bins=30, ax=axes[0, 2], alpha=0.7, label='No Churn', color='green')\n",
    "df[df['churn']==1]['tenure'].hist(bins=30, ax=axes[0, 2], alpha=0.7, label='Churn', color='red')\n",
    "axes[0, 2].set_title('Tenure Distribution by Churn')\n",
    "axes[0, 2].set_xlabel('Tenure (months)')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Monthly charges\n",
    "df[df['churn']==0]['monthly_charges'].hist(bins=30, ax=axes[1, 0], alpha=0.7, label='No Churn', color='green')\n",
    "df[df['churn']==1]['monthly_charges'].hist(bins=30, ax=axes[1, 0], alpha=0.7, label='Churn', color='red')\n",
    "axes[1, 0].set_title('Monthly Charges by Churn')\n",
    "axes[1, 0].set_xlabel('Monthly Charges ($)')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Churn by payment method\n",
    "pd.crosstab(df['payment_method'], df['churn']).plot(kind='bar', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Churn by Payment Method')\n",
    "axes[1, 1].set_xlabel('Payment Method')\n",
    "\n",
    "# Churn by internet service\n",
    "pd.crosstab(df['internet_service'], df['churn']).plot(kind='bar', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Churn by Internet Service')\n",
    "axes[1, 2].set_xlabel('Internet Service')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eda_churn_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… EDA visualizations saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19a4b3",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac937562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess the dataset for modeling\"\"\"\n",
    "    # Create a copy\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Drop customer_id (not a feature)\n",
    "    data = data.drop('customer_id', axis=1)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data.drop('churn', axis=1)\n",
    "    y = data['churn']\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    numerical_cols = ['tenure', 'age', 'monthly_charges', 'total_charges']\n",
    "    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "    \n",
    "    return X, y, label_encoders, scaler\n",
    "\n",
    "# Preprocess data\n",
    "print(\"ðŸ”„ Preprocessing data...\")\n",
    "X, y, label_encoders, scaler = preprocess_data(df)\n",
    "print(f\"âœ… Features shape: {X.shape}\")\n",
    "print(f\"âœ… Target shape: {y.shape}\")\n",
    "print(f\"âœ… Feature columns: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99019a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"ðŸ“Š Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"ðŸ“Š Training churn rate: {y_train.mean():.2%}\")\n",
    "print(f\"ðŸ“Š Test churn rate: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88d9229",
   "metadata": {},
   "source": [
    "## 5. Model Training with MLflow Tracking\n",
    "\n",
    "We'll train multiple models and track experiments in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e767eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(model, model_name, X_train, y_train, X_test, y_test, params=None):\n",
    "    \"\"\"Train model and log metrics to MLflow\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Log parameters\n",
    "        if params:\n",
    "            mlflow.log_params(params)\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"ðŸ”„ Training {model_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Get probabilities if available\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "            y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_pred_proba_train = y_pred_train\n",
    "            y_pred_proba_test = y_pred_test\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'train_accuracy': accuracy_score(y_train, y_pred_train),\n",
    "            'test_accuracy': accuracy_score(y_test, y_pred_test),\n",
    "            'train_precision': precision_score(y_train, y_pred_train),\n",
    "            'test_precision': precision_score(y_test, y_pred_test),\n",
    "            'train_recall': recall_score(y_train, y_pred_train),\n",
    "            'test_recall': recall_score(y_test, y_pred_test),\n",
    "            'train_f1': f1_score(y_train, y_pred_train),\n",
    "            'test_f1': f1_score(y_test, y_pred_test),\n",
    "            'train_auc': roc_auc_score(y_train, y_pred_proba_train),\n",
    "            'test_auc': roc_auc_score(y_test, y_pred_proba_test)\n",
    "        }\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Create and log confusion matrix plot\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        cm = confusion_matrix(y_test, y_pred_test)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "        ax.set_title(f'Confusion Matrix - {model_name}')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        plt.savefig(f'confusion_matrix_{model_name}.png')\n",
    "        mlflow.log_artifact(f'confusion_matrix_{model_name}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Log feature importance if available\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': X_train.columns,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            sns.barplot(data=importance_df.head(10), x='importance', y='feature', ax=ax)\n",
    "            ax.set_title(f'Top 10 Feature Importances - {model_name}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'feature_importance_{model_name}.png')\n",
    "            mlflow.log_artifact(f'feature_importance_{model_name}.png')\n",
    "            plt.close()\n",
    "        \n",
    "        print(f\"âœ… {model_name} trained successfully\")\n",
    "        print(f\"   Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "        print(f\"   Test F1: {metrics['test_f1']:.4f}\")\n",
    "        print(f\"   Test AUC: {metrics['test_auc']:.4f}\")\n",
    "        \n",
    "        return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e48b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "models_results = {}\n",
    "\n",
    "# 1. Logistic Regression\n",
    "lr_params = {'C': 1.0, 'max_iter': 1000, 'random_state': 42}\n",
    "lr_model, lr_metrics = train_and_log_model(\n",
    "    LogisticRegression(**lr_params),\n",
    "    \"Logistic_Regression\",\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    lr_params\n",
    ")\n",
    "models_results['Logistic Regression'] = lr_metrics\n",
    "\n",
    "print()\n",
    "\n",
    "# 2. Random Forest\n",
    "rf_params = {'n_estimators': 100, 'max_depth': 10, 'random_state': 42}\n",
    "rf_model, rf_metrics = train_and_log_model(\n",
    "    RandomForestClassifier(**rf_params),\n",
    "    \"Random_Forest\",\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    rf_params\n",
    ")\n",
    "models_results['Random Forest'] = rf_metrics\n",
    "\n",
    "print()\n",
    "\n",
    "# 3. Gradient Boosting\n",
    "gb_params = {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5, 'random_state': 42}\n",
    "gb_model, gb_metrics = train_and_log_model(\n",
    "    GradientBoostingClassifier(**gb_params),\n",
    "    \"Gradient_Boosting\",\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    gb_params\n",
    ")\n",
    "models_results['Gradient Boosting'] = gb_metrics\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b1ac7",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison_df = pd.DataFrame(models_results).T\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df[['test_accuracy', 'test_precision', 'test_recall', 'test_f1', 'test_auc']])\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "comparison_df[['train_accuracy', 'test_accuracy']].plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].legend(['Train', 'Test'])\n",
    "axes[0].set_ylim([0.5, 1.0])\n",
    "\n",
    "# F1 and AUC comparison\n",
    "comparison_df[['test_f1', 'test_auc']].plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Model F1 and AUC Comparison')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].legend(['F1 Score', 'AUC'])\n",
    "axes[1].set_ylim([0.5, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = comparison_df['test_f1'].idxmax()\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n",
    "print(f\"   Test F1 Score: {comparison_df.loc[best_model_name, 'test_f1']:.4f}\")\n",
    "print(f\"   Test AUC: {comparison_df.loc[best_model_name, 'test_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b1225",
   "metadata": {},
   "source": [
    "## 7. Register Best Model\n",
    "\n",
    "Register the best performing model to MLflow Model Registry for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80488c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register best model to Model Registry\n",
    "MODEL_NAME = \"customer_churn_model\"\n",
    "\n",
    "print(f\"ðŸ”„ Registering best model: {best_model_name}\")\n",
    "\n",
    "# Get the best model's run\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"metrics.test_f1 DESC\"],\n",
    "    max_results=1\n",
    ")\n",
    "\n",
    "if runs:\n",
    "    best_run = runs[0]\n",
    "    best_run_id = best_run.info.run_id\n",
    "    \n",
    "    # Register model\n",
    "    model_uri = f\"runs:/{best_run_id}/model\"\n",
    "    \n",
    "    try:\n",
    "        # Register model version\n",
    "        model_version = mlflow.register_model(model_uri, MODEL_NAME)\n",
    "        \n",
    "        print(f\"âœ… Model registered successfully\")\n",
    "        print(f\"   Model Name: {MODEL_NAME}\")\n",
    "        print(f\"   Version: {model_version.version}\")\n",
    "        print(f\"   Run ID: {best_run_id}\")\n",
    "        \n",
    "        # Transition to Production\n",
    "        client.transition_model_version_stage(\n",
    "            name=MODEL_NAME,\n",
    "            version=model_version.version,\n",
    "            stage=\"Production\",\n",
    "            archive_existing_versions=True\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Model transitioned to Production stage\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Model registration info: {str(e)}\")\n",
    "        print(f\"   This is expected on first run or if MLflow server is not fully initialized\")\n",
    "        print(f\"   Model URI: {model_uri}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No runs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a483ec1",
   "metadata": {},
   "source": [
    "## 8. Make Predictions with Registered Model\n",
    "\n",
    "Test loading and using the model from the registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a4b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from registry\n",
    "print(\"ðŸ”„ Loading model from registry...\")\n",
    "\n",
    "try:\n",
    "    loaded_model = mlflow.pyfunc.load_model(f\"models:/{MODEL_NAME}/Production\")\n",
    "    print(f\"âœ… Model loaded successfully from registry\")\n",
    "    \n",
    "    # Make sample predictions\n",
    "    sample_data = X_test.head(10)\n",
    "    predictions = loaded_model.predict(sample_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLE PREDICTIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Actual': y_test.head(10).values,\n",
    "        'Predicted': predictions,\n",
    "        'Match': y_test.head(10).values == predictions\n",
    "    })\n",
    "    \n",
    "    print(results)\n",
    "    print(f\"\\nAccuracy on sample: {results['Match'].mean():.2%}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not load model from registry: {str(e)}\")\n",
    "    print(\"   This is expected if model registration hasn't completed yet\")\n",
    "    print(\"   The model can still be accessed directly from the run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7754a",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "### âœ… What We Accomplished\n",
    "\n",
    "1. **Generated synthetic customer churn dataset** with realistic patterns\n",
    "2. **Performed EDA** to understand churn drivers\n",
    "3. **Preprocessed data** with encoding and scaling\n",
    "4. **Trained multiple models** (Logistic Regression, Random Forest, Gradient Boosting)\n",
    "5. **Tracked experiments** in MLflow with comprehensive metrics\n",
    "6. **Registered best model** to Model Registry\n",
    "7. **Tested model loading** from registry\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "1. **Access MLflow UI** at http://localhost:5000 to:\n",
    "   - Compare experiment runs\n",
    "   - View metrics and artifacts\n",
    "   - Manage model versions\n",
    "\n",
    "2. **Deploy model** using the FastAPI endpoint:\n",
    "   ```bash\n",
    "   # The model is automatically deployed via docker-compose\n",
    "   # Access API docs at: http://localhost:8000/docs\n",
    "   ```\n",
    "\n",
    "3. **Make predictions** via REST API:\n",
    "   ```python\n",
    "   import requests\n",
    "   \n",
    "   response = requests.post(\n",
    "       \"http://localhost:8000/predict\",\n",
    "       json={\"features\": {...}}\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. **Set up CI/CD** for automated retraining and deployment\n",
    "\n",
    "5. **Monitor model performance** in production\n",
    "\n",
    "### ðŸ’¡ Interview Tips\n",
    "\n",
    "When discussing this project:\n",
    "- Emphasize the **complete ML lifecycle** implementation\n",
    "- Highlight **experiment tracking** and **reproducibility**\n",
    "- Discuss **model versioning** and **deployment strategies**\n",
    "- Explain how this mirrors **Azure ML** workflows\n",
    "- Show understanding of **MLOps best practices**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7717ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸŽ‰ MLOps WORKFLOW COMPLETE!\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EXPERIMENT_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸŽ‰ MLOps WORKFLOW COMPLETE!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Experiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEXPERIMENT_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Best Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Model Registry: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EXPERIMENT_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ðŸŽ‰ MLOps WORKFLOW COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def _get(name, default):\n",
    "    return globals().get(name, default)\n",
    "\n",
    "EXPERIMENT_NAME_SAFE = _get(\"EXPERIMENT_NAME\", \"customer-churn-prediction\")\n",
    "BEST_MODEL_NAME_SAFE = _get(\"best_model_name\", \"(not computed yet â€” run training cells)\")\n",
    "MODEL_NAME_SAFE = _get(\"MODEL_NAME\", \"customer_churn_model\")\n",
    "\n",
    "print(f\"âœ… Experiment: {EXPERIMENT_NAME_SAFE}\")\n",
    "print(f\"âœ… Best Model: {BEST_MODEL_NAME_SAFE}\")\n",
    "print(f\"âœ… Model Registry: {MODEL_NAME_SAFE}\")\n",
    "print(f\"âœ… MLflow UI: http://localhost:5000\")\n",
    "print(f\"âœ… API Docs: http://localhost:8000/docs\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
